# -*- coding: utf-8 -*-
"""network_editor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PwK4lT0Ua_dJMaVMLLbXgZLSDpcaoJMa
"""

"""
defining all 1024 models
Network using pooling layers will mention Benâ€™s name
bens_maxunpool_1024
bens_nn_1024
bens_linear_1024
bens_transpose_1024

Network with strided convolution will mention strided:
strided_transpose_1024/256
strided_nn_1024/256
stided_linear_1024/256

All network have the same
kernel sizes for now:
For now, two 5s, six 3s, rest 1s
"""
import torch
from torch import nn
from torch.utils.data import Dataset
from scipy.stats import kurtosis,skew
import torch.nn.functional as F

class bens_cubic(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()

        #defines kernel and scale factor size centrally
        self.kernel_0 = 1
        self.kernel_1 = 5
        self.kernel_2 = 5
        self.kernel_3 = 3
        self.kernel_4 = 3
        self.kernel_5 = 3
        self.kernel_6 = 3
        self.kernel_7 = 3
        self.kernel_8 = 3
        self.kernel_9 = 1
        self.kernel_10 = 1
        self.kernel_11 = 1
        self.kernel_12 = 1
        self.kernel_13 = 1
        self.kernel_14 = 1
        self.kernel_15 = 1
        self.pool_1 = 4
        self.pool_2 = 2
        self.pool_3 = 2
        self.pool_4 = 2
        self.dense_1_in = 32
        self.dense_1_out = 32
        self.negative_slope = 0.1
        self.dropout = 0.1

        #required extra functions
        self.unflatten = nn.Unflatten(1,(1,32))

        """
        re-writing the convolutional layers
        in modular form

        Format:
        Convolutinal Layer
        Batch Normalization
        Activation
        """

        self.en_layer1 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_1, stride=1, padding = self.kernel_1//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer2 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_2, stride=1, padding = self.kernel_2//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer3 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_3, stride=1, padding = self.kernel_3//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer4 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_4, stride=1, padding = self.kernel_4//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer5 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_5, stride=1, padding = self.kernel_5//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer6 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_6, stride=1, padding = self.kernel_6//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer7 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_7, stride=1, padding = self.kernel_7//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer8 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_8, stride=1, padding = self.kernel_8//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer9 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_9, stride=1, padding = self.kernel_9//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer10 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_10, stride=1, padding = self.kernel_10//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer11 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_11, stride=1, padding = self.kernel_11//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer12 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_12, stride=1, padding = self.kernel_12//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer13 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_13, stride=1, padding = self.kernel_13//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.en_layer14 = nn.Sequential(
            nn.Conv1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_14, stride=1, padding = self.kernel_14//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )

        self.en_pool_1 = nn.MaxPool1d(
            kernel_size=self.pool_1
        )

        self.en_pool_2 = nn.MaxPool1d(
            kernel_size=self.pool_2
        )

        self.en_pool_3 = nn.MaxPool1d(
            kernel_size=self.pool_3
        )

        self.en_pool_4 = nn.MaxPool1d(
            kernel_size=self.pool_4
        )

        self.en_dropout = nn.Dropout(p=self.dropout)

        self.en_dense_1 = nn.Linear(
            in_features = self.dense_1_in, out_features = self.dense_1_out
        )

        """
        Defining all decoder layers here
        in a more modular format
        """

        self.de_layer1 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_1, stride=1, padding = self.kernel_1//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer2 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_2, stride=1, padding = self.kernel_2//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer3 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_3, stride=1, padding = self.kernel_3//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer4 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_4, stride=1, padding = self.kernel_4//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer5 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_5, stride=1, padding = self.kernel_5//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer6 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_6, stride=1, padding = self.kernel_6//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer7 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_7, stride=1, padding = self.kernel_7//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer8 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_8, stride=1, padding = self.kernel_8//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer9 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_9, stride=1, padding = self.kernel_9//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer10 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_10, stride=1, padding = self.kernel_10//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer11 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_11, stride=1, padding = self.kernel_11//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer12 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_12, stride=1, padding = self.kernel_12//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer13 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_13, stride=1, padding = self.kernel_13//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )
        self.de_layer14 = nn.Sequential(
            nn.ConvTranspose1d(
            in_channels=kwargs["input_shape"], out_channels=kwargs["input_shape"],
            kernel_size=self.kernel_14, stride=1, padding = self.kernel_14//2),
            nn.BatchNorm1d(num_features=kwargs["input_shape"]),
            nn.LeakyReLU(negative_slope = self.negative_slope)
        )

        self.de_dropout = nn.Dropout(p=self.dropout)

        self.de_dense_1 = nn.Linear(
            in_features = self.dense_1_out, out_features = self.dense_1_in
        )

    def forward(self, features):
      #Encoder
      activation = self.en_layer1(features)
      activation = self.en_layer2(features)

      activation = self.en_pool_1(activation)
      activation = self.en_layer3(activation)
      activation = self.en_layer4(activation)
      activation = self.en_layer5(activation)

      activation = self.en_pool_2(activation)

      activation = self.en_layer6(activation)
      activation = self.en_layer7(activation)

      activation = self.en_pool_3(activation)

      activation = self.en_layer8(activation)
      activation = self.en_layer9(activation)

      activation = self.en_pool_4(activation)

      activation = self.en_layer10(activation)
      activation = self.en_layer11(activation)
      activation = self.en_dropout(activation)
      activation = self.en_layer12(activation)
      activation = self.en_layer13(activation)
      activation = self.en_layer14(activation)

      activation = torch.flatten(activation, start_dim = 1)
      activation = self.en_dense_1(activation)
      activation = F.leaky_relu(self.en_dense_1(activation), negative_slope=self.negative_slope)
      activation = self.en_dropout(activation)
      activation = self.de_dense_1(activation)
      activation = F.leaky_relu(self.de_dense_1(activation), negative_slope=self.negative_slope)
      activation = self.de_dropout(activation)
      activation = self.unflatten(activation)

      activation = self.de_layer14(activation)
      activation = self.de_layer13(activation)
      activation = self.de_layer12(activation)
      activation = self.de_dropout(activation)
      activation = self.de_layer11(activation)
      activation = self.de_layer10(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_4, 1), mode="bicubic").squeeze(-1)

      activation = self.de_layer9(activation)
      activation = self.de_layer8(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_3, 1), mode="bicubic").squeeze(-1)

      activation = self.de_layer7(activation)
      activation = self.de_layer6(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_2, 1), mode="bicubic").squeeze(-1)

      activation = self.de_layer5(activation)
      activation = self.de_layer4(activation)
      activation = self.de_layer3(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_1, 1), mode="bicubic").squeeze(-1)


      activation = self.de_layer2(activation)
      activation = self.de_layer1(activation)
      reconstructed = activation
      return reconstructed
    
    def get_output_shape(self):
        return self.dense_1_out

class encode(bens_cubic):

    def forward(self, features):
      #Encoder
      activation = self.en_layer1(features)
      activation = self.en_layer2(features)

      activation = self.en_pool_1(activation)
      activation = self.en_layer3(activation)
      activation = self.en_layer4(activation)
      activation = self.en_layer5(activation)

      activation = self.en_pool_2(activation)

      activation = self.en_layer6(activation)
      activation = self.en_layer7(activation)

      activation = self.en_pool_3(activation)

      activation = self.en_layer8(activation)
      activation = self.en_layer9(activation)

      activation = self.en_pool_4(activation)

      activation = self.en_layer10(activation)
      activation = self.en_layer11(activation)
      activation = self.en_layer12(activation)
      activation = self.en_layer13(activation)
      activation = self.en_layer14(activation)

      activation = torch.flatten(activation, start_dim = 1)
      activation = self.en_dense_1(activation)
      activation = F.leaky_relu(activation, negative_slope=self.negative_slope)
      activation = self.en_dropout(activation)
      code = activation
      return code
    def get_output_shape(self):
       return super().get_output_shape()
    
class decode(bens_cubic):
    def forward(self, features):
      activation = self.de_dense_1(features)
      activation = F.leaky_relu(activation, negative_slope=self.negative_slope)
      activation = self.de_dropout(activation)
      activation = self.unflatten(activation)
      activation = self.de_layer14(activation)
      activation = self.de_layer13(activation)
      activation = self.de_layer12(activation)
      activation = self.de_layer11(activation)
      activation = self.de_layer10(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_4, 1), mode="bicubic").squeeze(-1)

      activation = self.de_layer9(activation)
      activation = self.de_layer8(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_3, 1), mode="bicubic").squeeze(-1)

      activation = self.de_layer7(activation)
      activation = self.de_layer6(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_2, 1), mode="bicubic").squeeze(-1)

      activation = self.de_layer5(activation)
      activation = self.de_layer4(activation)
      activation = self.de_layer3(activation)

      activation = F.interpolate(activation.unsqueeze(-1), scale_factor=(self.pool_1, 1), mode="bicubic").squeeze(-1)


      activation = self.de_layer2(activation)
      activation = self.de_layer1(activation)
      reconstructed = activation
      return reconstructed
